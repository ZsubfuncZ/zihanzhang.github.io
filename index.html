<html>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<head>
    <title>Zihan Zhang's Homepage</title>
    <style>
    #header {
        text-align: center;
        padding: 0px;
        margin-left: 150px;
    }

    #main {
        width:80%;
        float:left;
        padding:0px;
        margin-left:10%;        
    }
    #footer {
        clear:both;
        text-align:left;
        padding:5px;  
        margin-left:150px;     
    }
</style>
</head>

<body>
    <div id="header">
        <table cellspacing="25">
            <td>
                <img src="profile.png" width="200" height="200">    
            </td>
            <td>
                <h2>Zihan Zhang</h2>
                Postdoc<br>
                Paul G. Allen school of Computer Science & Engineering<br>
                University of Washington<br>
                <br>
                Email: zihanz46 at uw dot edu<br>
                <a href="https://scholar.google.com/citations?user=un0eGzEAAAAJ&hl=zh-CN" target="_blank">
                Google scholar<br></a>
                <br>
            </td>
        </table>
    </div>

    <div id="main">
        <p>
            I am a postdoc at <a href="https://www.cs.washington.edu/" target='_blank'>  Paul G. Allen school of Computer Science & Engineering </a> at University of Washington,  advised by <a href="https://simonshaoleidu.com/" target="_blank">  Simon S. Du</a>.

            Previously, I was a postdoc researcher at  <a href="https://ece.princeton.edu/" target='_blank'>  Department of Electrical and Computer Engineering </a> at Princeton University advised by <a href="https://yuxinchen2020.github.io/" target="_blank"> Yuxin Chen</a> and  <a href="https://jasondlee88.github.io/" target="_blank"> Jason D. Lee</a>. Before that, I received Ph.D and B.S. in engineering from <a href="https://www.au.tsinghua.edu.cn/en/" target="_blank"> Department of Automation</a>,  Tsinghua University, where I was supervised by <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm" target="_blank">Prof. Xiangyang Ji</a>.
        </p>

  

        <p>
          I have broad interests in machine learning theory, including reinforcement learning, online learning,  bandit theory and game theory. 
        </p>

        <p>
            <!--Here is my CV.-->
        </p>           
        
     <h2>Selected Works</h2>
            <ul>
                   <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2307.13586" target="_blank">
                <b>Settling the Sample Complexity of Online Reinforcement Learning</b></a>
                <br>
                <b>Zihan Zhang</b>, Yuxin Chen, Jason D. Lee, Simon S. Du.<br>
                <em>37th Annual Conference on Learning Theory (COLT)</em>, 2024.<br>
                We present a new model-based algorithm for learning in the fundamental paradigm of reinforcement learning: the tabular Markov Decision Process (MDP) setting. Our algorithm achieves a regret bound of $	\min\big\{  \sqrt{SAH^3K}, \,HK \big\}$  where  $S$ is the number of states, $A$ is the number of actions, $H$ is the number of horizons and $K$ is the sample of samples.
Notably, this is the first-ever result that matches the minimax lower bound for the entire range of sample size $K\geq 1$, settling a 30-year-old open problem.
                </li>

                <li>
                    <b>A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network</b><br>
                    COLT 2021, Aug. 2021<br>
                    Theory of Overparameterized Machine Learning (TOPML) 2021, Apr. 2021<br>
                    Duke Deep Learning Reading Group, Apr. 2021<br> 
                    THEORINET Journal Club/MODL Reading Group, Feb. 2021<br>
                </li>
                
                <li>
                    <b>A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network</b><br>
                    COLT 2021, Aug. 2021<br>
                    Theory of Overparameterized Machine Learning (TOPML) 2021, Apr. 2021<br>
                    Duke Deep Learning Reading Group, Apr. 2021<br> 
                    THEORINET Journal Club/MODL Reading Group, Feb. 2021<br>
                </li>
            </ul> 

        <h2>Publications and Preprints</h2>
            * denotes equal contribution,
            <ul>
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2406.01766" target="_blank">
                <b>How Does Gradient Descent Learn Features -- A Local Analysis for Regularized Two-Layer Neural Networks</b></a>
                <br>
                <b>Mo Zhou</b>, Rong Ge.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024.<br>
                Short version appeared at NeurIPS Mathematics of Modern Machine Learning (M3L) workshop, 2023.
                </li>

                <br>
                
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2302.00257" target="_blank">
                <b>Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression</b></a>
                <br>
                <b>Mo Zhou</b>, Rong Ge.<br>
                <em>International Conference on Machine Learning (ICML)</em>, 2023.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2210.03294" target="_blank">
                <b>Understanding Edge-of-Stability Training Dynamics with a Minimalist Example</b></a>
                <br>
                Xingyu Zhu*, Zixuan Wang*, Xiang Wang, <b>Mo Zhou</b>, Rong Ge.<br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2304.01063" target="_blank">    
                <b>Depth-Separation with Multilayer Mean-Field Networks.</b></a>
                <br>
                Yunwei Ren, <b>Mo Zhou</b>, Rong Ge.
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023. Notable-top-25%.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2210.01019" target="_blank">
                <b>Plateau in Monotonic Linear Interpolation–A “Biased” View of Loss Landscape for Deep Networks</b></a>
                <br>
                Xiang Wang, Annie N Wang, <b>Mo Zhou</b>, Rong Ge.
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2203.03539" target="_blank">
                <b>Understanding The Robustness of Self-supervised Learning Through Topic Modeling</b></a>
                <br>
                Zeping Luo*, Shiyou Wu*, Cindy Weng*, <b>Mo Zhou</b>, Rong Ge
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
                </li>
                
                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2106.06573" target="_blank">
                <b>Understanding Deflation Process in Over-parametrized Tensor Decomposition</b></a>
                <br>
                (&alpha;-&beta; order) Rong Ge*, Yunwei Ren*, Xiang Wang*, <b>Mo Zhou</b>*
                <br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2021.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2102.02410" target="_blank">
                <b>A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network</b></a>
                <!--[<a href="overparam_local.pptx">slides</a>]-->
                <br>
                <b>Mo Zhou</b>, Rong Ge, Chi Jin
                <br>
                <em>Conference on Learning Theory (COLT)</em>, 2021.
                </li>

                <br>

                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/1909.04653" target="_blank">
                <b>Towards Understanding the Importance of Shortcut Connections in Residual Networks</b></a>
                <br>
                Tianyi Liu*, Minshuo Chen*, <b>Mo Zhou</b>, Simon S. Du, Enlu Zhou, Tuo Zhao
                <br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019.
                </li>
                
                <br>
                
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/1909.03172" target="_blank">
                <b>Towards Understanding the Importance of Noise in Training Neural Networks</b></a>
                <br>
                <b>Mo Zhou</b>*, Tianyi Liu*, Yan Li, Dachao Lin, Enlu Zhou, Tuo Zhao
                <br>
                <em>International Conference on Machine Learning (ICML)</em>, 2019. Long Talk.
                </li>
            </ul>    

   
        <h2>Teaching</h2>
            <ul>
                <li>
                <a href="https://sites.google.com/view/duke-compsci-590-04-spring-202/home" target="_blank">
                <b>   Foundations of Machine Learning (80250943),  2018 Fall and 2019 Spring, Tsinghua University (Instructor: <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm" target="_blank">Prof. Xiangyang Ji</a>) </b></a>. TA
                </li>

                <br>


                <li>
                <a href="https://www2.cs.duke.edu/courses/fall20/compsci330/" target="_blank">
                <b>CPS330 Design and Analysis of Algorithms, 2020 Fall</b></a>. TA
                </li>

                <br>

                <li>
                <a href="https://www2.cs.duke.edu/courses/spring20/compsci330/" target="_blank">
                <b>CPS330 Design and Analysis of Algorithms, 2020 Spring</b></a>. TA
                </li>
                
            </ul>  


        <h2>Services</h2>
            <ul>
                <li>
                Reviewer for ICML, ICLR, NeurIPS, COLT, ALT, AISTATS.
                </li>
            </ul> 
        
 <h2>Working Experience</h2>
            <ul>
                <li>
                University of Washington,  Sep 2024 - Present<br>
                Postdoc in Paul G. Allen School of Computer Ccience and Engineering
                </li>
                
                <br>
                
                <li>
                Princeton University, Apri 2023 - Aug 2024<br>
                Postdoc in Department of Electrical and Computer Engineering
                </li>
            </ul>  

        
        <h2>Education</h2>
            <ul>
                <li>
                Tsinghua University,  Aug 2017 - Oct 2024<br>
                Ph.D. in Control Science and Engineering
                </li>
                
                <br>
                
                <li>
                Tsinghua University, Aug 2013 - July 2017<br>
                B.S. in Automation
                </li>
            </ul>  
    </div>


    <div id="footer">
        <p>
        Last update: Oct. 2024<br>
        <!--
        <div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=5r8n992u4ex&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;sx=0" async="async"></script></div>
        -->

        <a href="https://clustrmaps.com/site/1c1p3"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=8E8iZC4fXp9j2i9WmclWCAi6qqu2sw0xExxKmPGsIMc&cl=ffffff" /></a>
    </div>
</body>

</html>
