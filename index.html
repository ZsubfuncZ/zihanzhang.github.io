<html>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<head>
    <title>Zihan Zhang's Homepage</title>
    <style>
    #header {
        text-align: center;
        padding: 0px;
        margin-left: 150px;
    }

    #main {
        width:80%;
        float:left;
        padding:0px;
        margin-left:10%;        
    }
    #footer {
        clear:both;
        text-align:left;
        padding:5px;  
        margin-left:150px;     
    }
</style>
</head>

<body>
    <div id="header">
        <table cellspacing="25">
            <td>
                <img src="profile.png" width="200" height="200">    
            </td>
            <td>
                <h2>Zihan Zhang</h2>
                Postdoc<br>
                Paul G. Allen school of Computer Science & Engineering<br>
                University of Washington<br>
                <br>
                Email: zihanz46 at uw dot edu<br>
                <a href="https://scholar.google.com/citations?user=un0eGzEAAAAJ&hl=zh-CN" target="_blank">
                Google scholar<br></a>
                <br>
            </td>
        </table>
    </div>

    <div id="main">
        <p>
            I am a postdoc at <a href="https://www.cs.washington.edu/" target='_blank'>  Paul G. Allen school of Computer Science & Engineering </a> at University of Washington,  advised by <a href="https://simonshaoleidu.com/" target="_blank">  Simon S. Du</a>.

            Previously, I was a postdoc researcher at  <a href="https://ece.princeton.edu/" target='_blank'>  Department of Electrical and Computer Engineering </a> at Princeton University advised by <a href="https://yuxinchen2020.github.io/" target="_blank"> Yuxin Chen</a> and  <a href="https://jasondlee88.github.io/" target="_blank"> Jason D. Lee</a>. Before that, I received Ph.D and B.S. in engineering from <a href="https://www.au.tsinghua.edu.cn/en/" target="_blank"> Department of Automation</a> at  Tsinghua University, where I was supervised by <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm" target="_blank">Prof. Xiangyang Ji</a>.
        </p>

  

        <p>
          I have broad interests in machine learning theory, including reinforcement learning, online learning,  bandit theory and game theory. 
        </p>

        <p>
            <!--Here is my CV.-->
        </p>           
        
     <h2>Selected Works</h2>
            <ul>

               
                
                   <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2307.13586" target="_blank">
                <b>Settling the Sample Complexity of Online Reinforcement Learning</b></a>
                <br>
                <b>Zihan Zhang</b>, Yuxin Chen, Jason D. Lee, Simon S. Du.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2024.<br>
               <i> We present a new model-based algorithm for learning in the fundamental paradigm of reinforcement learning: the tabular Markov Decision Process (MDP) setting. Our algorithm achieves a regret bound of <math>	min<mi>{</mi>  <msqrt> <mi>SAH^3K</mi> </msqrt>, <mi>HK</mi> <mi>}</mi> </math>  where  <math><mi>S</mi></math> is the number of states, <math><mi>A</mi></math> is the number of actions, <math><mi>H</mi> </math> is the number of horizons and <math><mi>K</mi> </math> is the sample of samples.
Notably, this is the first-ever result that matches the minimax lower bound for the entire range of sample size <math><mi>K &#8805; 1 </mi> </math> , settling a 30-year-old open problem.</i>
                </li>

                <br>

        

                  <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2004.10019" target="_blank">
                <b>Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition</b></a>
                <br>
                <b>Zihan Zhang</b>, Yuan Zhou, Xiangyang Ji.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2020.<br>
               <i>We prove the first regret bound that matches the information-theoretic lower bound asymptotically with a model-free algorithm for finite-horizon MDP.
The reference-advantage technique developed in this paper has been widely used in the literature.</i>
                </li>

                <br>
                  <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2312.05134" target="_blank">
                <b>Optimal Multi-Distribution Learning</b></a>
                <br>
                <b>Zihan Zhang</b>, Wenhao Zhan, Yuxin Chen, Simon S. Du Jason D. Lee.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2024.<br>
          <i>We propose a novel algorithm that achieves a near-optimal sample complexity for learning multi-distributions, which settles an <a href="https://arxiv.org/pdf/2307.12135" target='_blank'>  open problem </a> in COLT 2023.</i>
                </li>

       <br>
                
 <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2411.17668" target="_blank">
                <b>Anytime Acceleration of Gradient Descent</b></a>
                <br>
                <b>Zihan Zhang</b>, Jason D. Lee, Simon S. Du, Yuxin Chen.<br>
                <em>preprint</em> <br>
               <i> For smooth (non-strongly) convex optimization, we propose a stepsize schedule that allows gradient descent to achieve convergence guarantees of O(T^{-1.03}) for any stopping time  T, 
where the stepsize schedule is predetermined without prior knowledge of the stopping time. This result provides an affirmative answer to  a COLT 2024 <a href="https://proceedings.mlr.press/v247/kornowski24a.html" target='_blank'>  open problem </a>  regarding whether stepsize-based acceleration  can yield anytime convergence rates of o(T^{-1}). </i>
                </li>

         

                
            </ul> 

        <h2>Publications and Preprints</h2>
            * denotes equal contribution,
            <ul>
                <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2406.01234" target="_blank">
                <b>Achieving Tractable Minimax Optimal Regret in Average Reward MDPs</b></a>
                <br>
                Victor Boone, <b>Zihan Zhang</b>.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024.<br>
                </li>

                <br>

                 <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2307.13586" target="_blank">
                <b>Settling the Sample Complexity of Online Reinforcement Learning</b></a>
                <br>
               <b>Zihan Zhang</b>, Yuxin Chen, Jason D. Lee, Simon S. Du.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2024.<br>
                </li>

                <br>

                  <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2312.05134" target="_blank">
                <b>Optimal Multi-Distribution Learning</b></a>
                <br>
               <b>Zihan Zhang</b>, Wenhao Zhan, Yuxin Chen, Simon S. Du, Jason D. Lee.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2024.<br>
                </li>

                <br>

                
                  <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2403.10738" target="_blank">
                <b>Horizon-Free Regret for Linear Markov Decision Processes</b></a>
                <br>
               <b>Zihan Zhang</b>, Jason D. Lee, Yuxin Chen, Simon S. Du.<br>
                <em> International Conference on Learning Representations (ICLR)</em>, 2024.<br>
                </li>

                   <br>

                  <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2306.16394" target="_blank">
                <b>Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes</b></a>
                <br>
               <b>Zihan Zhang</b>, Qiaomin Xie.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2023.<br>
                </li>

                <br>

                    <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2301.13446" target="_blank">
                <b>Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments</b></a>
                <br> Runlong Zhou, 
               <b>Zihan Zhang</b>, Simon S. Du.<br>
                <em>International Conference on Machine Learning (ICML)</em>, 2023.<br>
                </li>

                <br>

                      <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2210.08238" target="_blank">
                <b>Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning</b></a>
                <br>
                <b>Zihan Zhang</b>, Yuhang Jiang, Yuan Zhou, Xiangyang Ji.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2022.<br>
                </li>

                <br>

                     <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2203.12922" target="_blank">
                <b>Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies</b></a>
                <br>
               <b>Zihan Zhang</b>, Xiangyang Ji, Simon S. Du.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2022.<br>
                </li>

                <br>

                          <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2101.12745" target="_blank">
                <b>Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP</b></a>
                <br>
                <b>Zihan Zhang</b>*, Jiaqi Yang*, Xiangyang Ji, Simon S. Du.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2021.<br>
                </li>

                <br>
                        <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2009.13503" target="_blank">
                <b>Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon</b></a>
                <br>
               <b>Zihan Zhang</b>, Xiangyang Ji, Simon S. Du.<br>
                <em>Annual Conference on Learning Theory (COLT)</em>, 2021.<br>
                </li>

                <br>
                 <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2006.03864" target="_blank">
                <b>Model-Free Reinforement Learning: from Pseudo-Regret to Sample Complexity</b></a>
                <br>
               <b>Zihan Zhang</b>, Yuan Zhou, Xiangyang Ji.<br>
                <em>International Conference on Machine Learning (ICML)</em>, 2021.<br>
                </li>

                <br>

                     <li>
                <a style="text-decoration: none" href="https://proceedings.mlr.press/v139/zhang21e.html" target="_blank">
                <b>Near Optimal Reward-Free Reinforcement Learning</b></a>
                <br>
               <b>Zihan Zhang</b>, Simon S. Du, Xiangyang Ji.<br>
                <em>International Conference on Machine Learning (ICML)</em>, 2021.<br>
                </li>

                <br>

                         <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/2004.10019" target="_blank">
                <b>Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition</b></a>
                <br>
                <b>Zihan Zhang</b>, Yuan Zhou, Xiangyang Ji.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2020.<br>
                </li>

                <br>

                   <li>
                <a style="text-decoration: none" href="https://arxiv.org/abs/1906.05110" target="_blank">
                <b>Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function</b></a>
                <br>
                <b>Zihan Zhang</b>, Xiangyang Ji.<br>
                <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2019.<br>
                </li>


                

            </ul>    

   
        <h2>Teaching</h2>
            <ul>
                <li>
                  Foundations of Machine Learning (80250943),  2018 Fall and 2019 Spring, Tsinghua University (Instructor: <a href="https://www.au.tsinghua.edu.cn/info/1080/3178.htm" target="_blank">Prof. Xiangyang Ji</a>) . TA
                </li>
                
            </ul>  


        <h2>Services</h2>
            <ul>
                <li>
                Reviewer for ICML, ICLR, NeurIPS, COLT, ALT and AISTATS.
                </li>
            </ul> 
        
 <h2>Working Experience</h2>
            <ul>
                <li>
                University of Washington,  Sep 2024 - Present<br>
                Postdoc in Paul G. Allen School of Computer Ccience and Engineering
                </li>
                
                <br>
                
                <li>
                Princeton University, Apri 2023 - Aug 2024<br>
                Postdoc in Department of Electrical and Computer Engineering
                </li>
            </ul>  

        
        <h2>Education</h2>
            <ul>
                <li>
                Tsinghua University,  Aug 2017 - Oct 2022<br>
                Ph.D. in Control Science and Engineering
                </li>
                
                <br>
                
                <li>
                Tsinghua University, Aug 2013 - July 2017<br>
                B.S. in Automation
                </li>
            </ul>  
    </div>


    <div id="footer">
        <p>
        Last update: Nov. 2024<br>
    </div>
</body>

</html>
